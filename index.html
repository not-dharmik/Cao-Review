<!DOCTYPE html>
<html>
<head>
    <title>Google TPU!</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-F3w7mX95PdgyTmZZMECAngseQB83DfGTowi0iMjiWaeVhAn4FJkqJByhZMI3AhiU" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.9.1/dist/umd/popper.min.js" integrity="sha384-SR1sx49pcuLnqZUnnPwx6FCym0wLsk5JZuNx2bPPENzswTNFaQU1RDvt3wT4gWFG" crossorigin="anonymous"></script>
    <style type="text/css">
    
    </style>
</head>
<body>
    <!-- NAVBAR -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
        <div class="container">
          <a class="navbar-brand" href="#">
            <img src="https://logos-world.net/wp-content/uploads/2021/02/Google-Cloud-Emblem.png"  height="26">
            TPU
          </a>
        </div>
    </nav>

    <!-- Content -->
    <!-- Q1 -->
    <br>
    <div class="container">
        <h1>What is the difference between CPU, GPU, TPU?</h1>
        <p>
            <h4>
                The difference between CPU, GPU and TPU is that the CPU handles all the logics, calculations, and input/output of the computer, it is a general-purpose processor. In comparison, GPU is an additional processor to enhance the graphical interface and run high-end tasks. TPUs are powerful custom-built processors to run the project made on a specific framework, i.e. TensorFlow.
                <br>
                <ul>
                    <li> <strong>CPU:</strong>  Central Processing Unit. Manage all the functions of a computer.</li>
                </ul>
                <ul>
                    <li> <strong>GPU:</strong>  Graphical Processing Unit. Enhance the graphical performance of the computer.</li>
                </ul>
                <ul>
                    <li> <strong>TPU:</strong> Tensor Processing Unit. Custom build ASIC to accelerate TensorFlow projects.</li>
                </ul>
            </h4>
        </p>  
        <br>
    </div>
    <!--  -->
    <div class="container">
        <div class="container" style="width: 50%;float: left;">
            <p>
                <h1>What is Tensor?</h1>
            </p>
            <p>
                <h4>
                    a tensor is a geometric object that maps geometric vectors, scalars, and other 
                    similar objects in a multi-linear manner to a resulting tensor. This may sound 
                    quite confusing, but in simpler words, a tensor is a generalized matrix that could 
                    be a 1-D matrix (a vector), a 3-D matrix (a cube of numbers), even a 0-D matrix (a single number), 
                    or a higher dimensional structure that is harder to visualize.
                </h4>
            </p>
        </div>
        <div class="container" style="width: 10% ;float: left;">
        </div>
        <div class="container" style="width: 50%; float: left;">
            <img src="https://miro.medium.com/max/449/1*kHues3bfBOythrXNLosesQ.png" width="100%">
        </div>
    </div>
    <div class="container">
        
          
        <br>
    </div>
    <!-- Q2 -->
    <div class="container">
        
        <h1>How does it work?</h1>
        <p>
            <h4>
                <p>
                    Many of the most impressive artificial intelligence (AI) breakthroughs over the past several years have been achieved with so-called deep neural networks. The behavior of these networks is loosely inspired by findings from neuroscience, but the term neural network is now applied to a broad class of mathematical structures that are not constrained to match biological findings. Many of the most popular neural network structures, or architectures, are organized in a hierarchy of layers. The most accurate and useful models tend to contain many layers, which is where they get the term deep. Most of these deep neural networks accept input data, such as images, audio, text, or structured data, apply a series of transformations, and then produce output that can be used to make predictions.
                </p>
            </h4>
        </p>
    </div>
    <!-- Q3 -->
    <div class="container">
        <div class="container" style="width: 50%;float: left;">
            <p>
                <h4>
                    When Google designed the TPU, we built a domain-specific architecture. That means, instead of designing a general purpose processor, we designed it as a matrix processor specialized for neural network work loads. TPUs can't run word processors, control rocket engines, or execute bank transactions, but they can handle the massive multiplications and additions for neural networks, at blazingly fast speeds while consuming much less power and inside a smaller physical footprint.
                </h4>
            </p>
        </div>
        <div class="container" style="width: 10% ;float: left;">
        </div>
        <div class="container" style="width: 50%; float: left;">
            <img src="https://cloud.google.com/tpu/docs/images/image8_qnuvyd4.png" width="100%">
        </div>
        <div class="container" style="float: left; width: 100%;">
            <p>
                <h4>
                    The key enabler is a major reduction of the von Neumann bottleneck. Because the primary task for this processor is matrix processing, hardware designer of the TPU knew every calculation step to perform that operation. So they were able to place thousands of multipliers and adders and connect them to each other directly to form a large physical matrix of those operators. This is called systolic array architecture. In case of Cloud TPU v2, there are two systolic arrays of 128 x 128, aggregating 32,768 ALUs for 16 bit floating point values in a single processor.
                </h4>
            </p>
            <p>
                <h4>
                    By matching its weights against the input it receives, the neuron is acting as a similarity filter, as illustrated here:
                </h4>
            </p>
            <p>
                <center>
                    <img src="https://cloud.google.com/tpu/docs/images/image3_aovcb32.gif">
                </center>       
            </p>
            <p>
                <h4>
                    While this is a basic example, it illustrates the core behavior of much more complex neural networks containing many 
                    layers that perform many intermediate transformations of the input data they receive. At each layer, the incoming input 
                    data, which may have been heavily altered by preceding layers, is matched against the weights of each neuron in the network. 
                    Those neurons then pass along their responses as input to the next layer.
                </h4>
            </p>
            <p>
                <h4>
                    How are these weights calculated for every neuron set? That takes place through a training process. This process often involves 
                    processing very large labeled datasets over and over. These datasets may contain millions or even billions of labeled examples! 
                    Training state-of-the art ML models on large datasets can take weeks even on powerful hardware. Google designed and built TPUs 
                    to increase productivity by making it possible to complete massive computational workloads like these in minutes or hours instead 
                    of weeks.
                </h4>
            </p>
            <p>
                <h1>
                    How a CPU works?
                </h1>
            </p>
            <p>
                <h4>
                    The last section provided a working definition of neural networks and the type of computations they involve. To understand the TPU's role in these networks, it helps to understand how other hardware devices address these computational challenges. To start, consider the CPU.
                </h4>
                <h4>    
                    The CPU is a general purpose processor based on the von Neumann architecture. That means a CPU works with software and memory, like this:
                </h4>
            </p>
            <p>
                <center>
                    <img src="https://cloud.google.com/tpu/docs/images/image6.gif">
                </center>
            </p>
            <p>
                <h4>
                    The greatest benefit of CPU is its flexibility. With its von Neumann architecture, you can load any kind of software for millions of 
                    different applications. You could use a CPU for word processing in a PC, controlling rocket engines, executing bank transactions, or 
                    classifying images with a neural network.
                </h4>
            </p>
            <p>
                <h4>
                    But, because the CPU is so flexible, the hardware doesn't always know what the next calculation is until it reads the next instruction 
                    from the software. A CPU has to store the calculation results inside the CPU registers or L1 cache for every single calculation. This 
                    memory access becomes the downside of CPU architecture called the von Neumann bottleneck. The huge scale of neural network calculations 
                    means that these future steps are entirely predictable. Each CPU's Arithmetic Logic Units (ALUs), which are the components that hold and 
                    control multipliers and adders, can execute only one calculation at a time. Each time, the CPU has to access memory, which limits the 
                    total throughput and consumes significant energy.
                </h4>
            </p>
            <p>
                <h1>
                    How a GPU works?
                </h1>
            </p>
            <p>
                <h4>
                    To gain higher throughput than a CPU, a GPU uses a simple strategy: employ thousands of ALUs in a single processor. In fact, the modern 
                    GPU usually has between <strong> 2,500–5,000 ALUs </strong>  in a single processor. This large number of processors means you could execute thousands of 
                    multiplications and additions simultaneously.
                </h4>
            </p>
            <p>
                <center>
                    <img src="https://cloud.google.com/tpu/docs/images/image2.gif">
                </center>
            </p>
            <p>
                <h4>
                    This GPU architecture works well on applications with massive parallelism, such as matrix multiplication in a neural network. 
                    In fact, on a typical training workload for deep learning, a GPU can provide an order of magnitude higher throughput than a 
                    CPU. This is why the GPU is the most popular processor architecture used in deep learning.
                </h4>
            </p>
            <p>
                <h4>
                    But, the GPU is still a general purpose processor that has to support millions of different applications and software. 
                    This means GPUs have the same problem as CPUs: the von Neumann bottleneck. For every single calculation in the thousands 
                    of ALUs, a GPU must access registers or shared memory to read and store the intermediate calculation results. Because the 
                    GPU performs more parallel calculations on its thousands of ALUs, it also spends proportionally more energy accessing memory, 
                    which increases the footprint of GPU for complex wiring.
                </h4>
            </p>

            <p>
                <h1>
                    How a TPU works?
                </h1>
            </p>
            <p>
                <h4>
                    Google designed Cloud TPUs as a matrix processor specialized for neural network workloads. TPUs can't run word processors, 
                    control rocket engines, or execute bank transactions, but they can handle the massive multiplications and additions for neural 
                    networks at very fast speeds while consuming much less power and inside a smaller physical footprint.
                </h4>
            </p>
            <p>
                <h4>
                    One benefit TPUs have over other devices is a major reduction of the von Neumann bottleneck. Because the primary task for this 
                    processor is matrix processing, hardware designers of the TPU knew every calculation step to perform that operation. So they 
                    were able to place thousands of multipliers and adders and connect them to each other directly to form a large physical matrix 
                    of those operators. This is called a systolic array architecture. In the case of Cloud TPU v2, there are two systolic arrays of 
                    128 x 128, aggregating 32,768 ALUs for 16 bit floating point values in a single processor.
                </h4>
            </p>
            <p>
                <h4>
                    Let's see how a systolic array executes the neural network calculations. At first, the TPU loads the parameters from memory 
                    into the matrix of multipliers and adders.
                </h4>
            </p>
            <p>
                <center>
                    <img src="https://cloud.google.com/tpu/docs/images/image4_5pfb45w.gif">
                </center>
            </p>
            <p>
                <h4>
                    Then, the TPU loads data from memory. As each multiplication is executed, the result will be passed to the next multipliers 
                    while taking the summation at the same time. So the output will be the summation of all multiplication results between data 
                    and parameters. During the whole process of massive calculations and data passing, no memory access is required at all.
                </h4>
            </p>
            <p>
                <center>
                    <img src="https://cloud.google.com/tpu/docs/images/image1_2pdcvle.gif">
                </center>
            </p>
            <p>
                <h4>
                    As a result, TPUs can achieve a high computational throughput on neural network calculations with much less power
                     consumption and smaller footprint.
                </h4>
            </p>
            <p>
                <h1>
                    The benefit: the cost reduces to one fifth
                </h1>
            </p>
            <p>
                <h4>
                    So what's the benefit you could get with this TPU architecture? The answer is cost. The following is the pricing of 
                    Cloud TPU v2 in August 2018, at the time of writing:
                </h4>
            </p>
            <p>
                <center>
                    <img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image5_w6RQJ3b.max-900x900.png">
                </center>
            </p>
            <p>
                <h4>
                    Stanford University publishes DAWNBench, which is a benchmark suite for deep learning training and inference. 
                    You can find various different combinations of tasks, models, and computing platforms and their respective benchmark results.
                </h4>
            </p>
            <p>
                <h4>
                    At the time DAWNBench contest closed on April 2018, the lowest training cost by non-TPU processors was $72.40 
                    (for training ResNet-50 at 93% accuracy with ImageNet using spot instance). With Cloud TPU v2 pre-emptible pricing, you can 
                    finish the same training at $12.87. It's less than 1/5th of non-TPU cost. This is the power of domain specific architecture 
                    for neural network.
                </h4>
            </p>
        </div>
    </div>

    

    
<!-- footer --> 


<div style="height: 800px;">
    
    <div class="container">
        <div class="row bg-dark">
            <div>
                <h1 style="color: cornsilk; text-align: center;">
                    --------------------------------------------------------------------
                </h1>
            </div>
            <div>
                <h1 style="color: cornsilk; text-align: center;">
                    Made by &#10084; from Dharmik (20BCB0148)
                </h1>
            </div>
            <div>
                <h1 style="color: cornsilk; text-align: center;">
                    --------------------------------------------------------------------
                </h1>
            </div>
            <div class="col-3 "></div>
            <div class="col-1 ">
                <a href="https://www.instagram.com/not.dharmik/">
                    <svg xmlns="http://www.w3.org/2000/svg" width="50" height="50" fill="currentColor" class="bi bi-instagram " viewBox="0 0 16 16">
                        <path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.917 3.917 0 0 0-1.417.923A3.927 3.927 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.916 3.916 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.926 3.926 0 0 0-.923-1.417A3.911 3.911 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0h.003zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599.28.28.453.546.598.92.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.47 2.47 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.478 2.478 0 0 1-.92-.598 2.48 2.48 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233 0-2.136.008-2.388.046-3.231.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92.28-.28.546-.453.92-.598.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045v.002zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92zm-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217zm0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334z"/>
                    </svg>
                </a>
            </div>
            <div class="col-1"></div>
            <div class="col-1">
                <a href="https://github.com/not-dharmik">
                    <svg xmlns="http://www.w3.org/2000/svg" width="50" height="50" fill="currentColor" class="bi bi-github" viewBox="0 0 16 16">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
                    </svg>
                </a> 
            </div>
            <div class="col-1 "></div>
            <div class="col-1 ">
                <a href="https://www.facebook.com/people/Dharmik-Naicker/100005041698476/">
                    <svg xmlns="http://www.w3.org/2000/svg" width="50" height="50" fill="currentColor" class="bi bi-facebook" viewBox="0 0 16 16">
                        <path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951z"/>
                    </svg>
                </a>
            </div>
            <div class="col-1 "></div>
            <div class="col-1 ">
                <a href="https://www.linkedin.com/in/dharmik-naicker-b3bb97202  ">
                    <svg xmlns="http://www.w3.org/2000/svg" width="50" height="50" fill="currentColor" class="bi bi-linkedin" viewBox="0 0 16 16">
                        <path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4z"/>
                    </svg>
                </a>
            </div>
            <div>
                <h1 style="color: cornsilk; text-align: center;">
                    --------------------------------------------------------------------
                </h1>
            </div>
            <br><br>
        </div>
    </div>
    <br>    
</div>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-/bQdsTh/da6pkI1MST/rWKFNjaCP5gBSY4sEBT38Q/9RBh9AH40zEOg7Hlq2THRZ" crossorigin="anonymous"></script>
</body>
</html>